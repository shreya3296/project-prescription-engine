{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tEMp54hWzkcH"
      },
      "outputs": [],
      "source": [
        "!pip install pandas regex scikit-learn transformers torch google-cloud-storage"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import numpy as np\n",
        "import os\n",
        "from google.colab import auth\n",
        "from google.cloud import storage\n",
        "import json\n",
        "from pandas import testing\n",
        "import time\n",
        "import requests\n",
        "import re\n",
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "Z__-UZPTFe7D"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Authenticate with GCP\n",
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/content/steel-climber-398320-634bf855feea.json'"
      ],
      "metadata": {
        "id": "_DIe8jw4FhLV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_folder(bucket_name, source_folder, destination_dir):\n",
        "    \"\"\"Downloads a folder from the bucket.\"\"\"\n",
        "    try:\n",
        "        storage_client = storage.Client('steel-climber-398320')  # Project of the bucket. Must be included\n",
        "        bucket = storage_client.bucket(bucket_name)\n",
        "\n",
        "        # List all blobs in the bucket\n",
        "        blobs = bucket.list_blobs(prefix=source_folder)  # Add prefix to optimize listing\n",
        "\n",
        "        for blob in blobs:\n",
        "            if not blob.name.endswith('/'):  # Skip directories\n",
        "                # Construct the local file path\n",
        "                local_file_path = os.path.join(destination_dir, blob.name[len(source_folder):])\n",
        "                # Create local path folders\n",
        "                os.makedirs(os.path.dirname(local_file_path), exist_ok=True)\n",
        "                # Download file by file\n",
        "                blob.download_to_filename(local_file_path)\n",
        "                # Keep track\n",
        "                print(f\"Blob {blob.name} downloaded to {local_file_path}.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred: {e}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "ewxKbAiZFkiG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#first, scrap the information from medscape\n",
        "\n",
        "url = 'https://reference.medscape.com/drugs'\n",
        "response = requests.get(url)\n",
        "if response.status_code != 200:\n",
        "    print(\"Failed to retrieve the main webpage\")\n",
        "response.status_code\n",
        "\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "category_names = []\n",
        "category_urls = []\n",
        "categories = soup.select('ul.classdruglist li a')\n",
        "\n",
        "all_data = []\n",
        "for category in categories:\n",
        "    category_names.append(category.get_text(strip=True))\n",
        "    category_urls.append(category['href'])\n",
        "\n",
        "df1 = pd.DataFrame({\n",
        "    'Category Name': category_names,\n",
        "    'Category URL': category_urls\n",
        "})"
      ],
      "metadata": {
        "id": "UjH3QZkMzjrA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_drugs_df = pd.DataFrame()\n",
        "\n",
        "# loop over the rows in df1\n",
        "for _, row in df1.iterrows():\n",
        "    category_name = row['Category Name']\n",
        "    url = row['Category URL']\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        html_content = response.text\n",
        "        pattern = r'<div class=\"topic-head\" data-id=\".*?\">(.*?)</div>.*?<a href=\"(.*?)\">(.*?)</a>'\n",
        "        matches = re.findall(pattern, html_content, re.DOTALL)\n",
        "        df_drugs = pd.DataFrame(matches, columns=['Class', 'Link', 'Drug'])\n",
        "        df_drugs['Category'] = category_name\n",
        "        all_drugs_df = pd.concat([all_drugs_df, df_drugs], ignore_index=True)\n",
        "\n",
        "    else:\n",
        "        print(f\"Failed to retrieve the webpage for category: {category_name}\")\n",
        "\n",
        "    # sleep for 1seg to avoid overwhelming the server\n",
        "    time.sleep(1)"
      ],
      "metadata": {
        "id": "Sm8Ju_K9zjfo"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function to extract conditions and doses\n",
        "def extract_conditions_and_doses(html_content):\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "    tags = soup.find_all(['h3', 'p'])\n",
        "    condition_doses = {}\n",
        "    dose_pattern = r'\\b\\d+(\\.\\d+)?\\s*(mg|g|ml|mL|mcg|L|IU)\\b|\\bq(Day|Week|Hour|Month)\\b'\n",
        "\n",
        "    current_condition = None\n",
        "    dose_info = ''\n",
        "\n",
        "    for tag in tags:\n",
        "        if tag.name == 'h3':\n",
        "            # if we have accumulated dose info, save it before moving to the next condition\n",
        "            if current_condition and dose_info:\n",
        "                condition_doses[current_condition] = dose_info.strip()\n",
        "                dose_info = ''\n",
        "\n",
        "            # update the current condition\n",
        "            current_condition = tag.get_text().strip()\n",
        "\n",
        "        elif tag.name == 'p' and current_condition:\n",
        "            # check if the paragraph contains dosing information\n",
        "            if re.search(dose_pattern, tag.get_text(), re.IGNORECASE):\n",
        "                dose_info += tag.get_text().strip() + ' '\n",
        "\n",
        "    # add the last accumulated dose info\n",
        "    if current_condition and dose_info:\n",
        "        condition_doses[current_condition] = dose_info.strip()\n",
        "\n",
        "    return condition_doses"
      ],
      "metadata": {
        "id": "Kby00h6Xzjbp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = []\n",
        "\n",
        "# iterate over each row in the all_drugs_df data frame\n",
        "for index, row in all_drugs_df.iterrows():\n",
        "    drug_name = row['Drug']\n",
        "    html_link = row['Link']\n",
        "\n",
        "    response = requests.get(html_link)\n",
        "    html_content = response.text\n",
        "\n",
        "    # extract conditions and doses\n",
        "    extracted_data = extract_conditions_and_doses(html_content)\n",
        "\n",
        "    # append the data to the list\n",
        "    for condition, dose in extracted_data.items():\n",
        "        # remove comments in parentheses from condition name\n",
        "        clean_condition = re.sub(r'\\(.*?\\)', '', condition).strip()\n",
        "        data.append([drug_name, condition, clean_condition, dose])\n",
        "\n",
        "# create a new data frame from the list\n",
        "prescriptions = pd.DataFrame(data, columns=['Drug Name', 'Indication', 'Clean Indication', 'Dosing Information'])\n"
      ],
      "metadata": {
        "id": "OlZc25kmzjYB"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get icd10 codes embeded\n",
        "bucket_name = 'embedded_data'\n",
        "source_folder = \"icd_10/\"\n",
        "destination_dir = \"/content/data\"\n",
        "download_folder(bucket_name, source_folder, destination_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjRz0-_I5Ev7",
        "outputId": "e3acd790-d95b-4f46-8ac4-5523d3e4328a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Blob icd_10/icd10_embed.json downloaded to /content/data/icd10_embed.json.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#get the final_dataframe file to have the icd10 codes and diagnoses being predicted\n",
        "bucket_name = 'scraped_data_clean'\n",
        "source_folder = \"icd_10_codes_diagnosis/\"\n",
        "destination_dir = \"/content/data\"\n",
        "download_folder(bucket_name, source_folder, destination_dir)\n",
        "\n",
        "#load the dataset\n",
        "file_path = '/content/data/final_dataframe.csv'\n",
        "icd_10_diagnosis_used = pd.read_csv(file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYJcyE-rJb6-",
        "outputId": "f3598dcf-b7af-439c-93ab-acc3368a0322"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Blob icd_10_codes_diagnosis/final_dataframe.csv downloaded to /content/data/final_dataframe.csv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# function to remove upper cases and strange symbols\n",
        "def preprocess_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove special characters (keeping spaces)\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "AbJYKkcp0KFB"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply preprocessing to the diagnosis that will be embedded\n",
        "prescriptions['indication_to_embed'] = prescriptions['Clean Indication'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "RBMeKLBD0KHf"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load tokenizer and model for bioBERT\n",
        "tokenizer = BertTokenizer.from_pretrained('dmis-lab/biobert-v1.1')\n",
        "model = BertModel.from_pretrained('dmis-lab/biobert-v1.1')"
      ],
      "metadata": {
        "id": "RnUI_ttA0KMj"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to create the embeddings of the diagnosis and descriptions\n",
        "def text_to_embedding(text):\n",
        "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
        "    outputs = model(**inputs)\n",
        "    embedding = outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
        "    assert isinstance(embedding, np.ndarray)\n",
        "    return embedding"
      ],
      "metadata": {
        "id": "pWdPBrii0KPK"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get embeddings\n",
        "prescriptions_embeddings = prescriptions['indication_to_embed'].apply(text_to_embedding)"
      ],
      "metadata": {
        "id": "51AdQ1E_0KRw"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create and embeddings column for future use in the dashboard\n",
        "prescriptions['indication_embedding'] = prescriptions_embeddings"
      ],
      "metadata": {
        "id": "MPZRGQmr0KUP"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load the icd_10s already embeded\n",
        "file_path = '/content/data/icd10_embed.json'\n",
        "icd10_embeddings = []\n",
        "with open(file_path, 'r') as file:\n",
        "    for line in file:\n",
        "        try:\n",
        "            icd10_embeddings.append(json.loads(line))\n",
        "        except json.JSONDecodeError:\n",
        "            print(f\"Error decoding JSON from line: {line}\")\n",
        "\n",
        "icd10_embeddings  = pd.DataFrame(icd10_embeddings)"
      ],
      "metadata": {
        "id": "v0y20XF45Tyg"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "icd_10_diagnosis_used = pd.merge(icd_10_diagnosis_used, icd10_embeddings, left_on = 'Matched_Code', right_on='code', how='left')\n"
      ],
      "metadata": {
        "id": "Q_uY23uoNJ9I"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "diagnosis_embedding_column = icd_10_diagnosis_used['diagnosis_embedding']\n",
        "\n",
        "# apply the function to each element in the column\n",
        "processed_arrays = diagnosis_embedding_column.apply(lambda x: np.array(x).reshape(1, -1))\n",
        "\n",
        "# vertically stack the resulting arrays\n",
        "icd10_embeddings_array = np.vstack(processed_arrays)"
      ],
      "metadata": {
        "id": "asSQiRoO18jl"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert embeddings in prescriptions DataFrame to a matrix\n",
        "prescription_embeddings_matrix = np.vstack(prescriptions['indication_embedding'].apply(np.array))"
      ],
      "metadata": {
        "id": "EuTFG1-OREU_"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# nitialize an empty list to store the diagnosis-prescription pairs\n",
        "diagnosis_prescription_pairs = []"
      ],
      "metadata": {
        "id": "cdJrBt-WQ5z8"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to find top 5 matching prescriptions for a given diagnosis embedding\n",
        "def find_top_5_matches(diagnosis_embedding, diagnosis_row):\n",
        "    # Reshape diagnosis embedding for cosine similarity calculation\n",
        "    diagnosis_embedding_reshaped = np.array(diagnosis_embedding).reshape(1, -1)\n",
        "\n",
        "    # Calculate cosine similarity with all prescriptions\n",
        "    similarity_scores = cosine_similarity(diagnosis_embedding_reshaped, prescription_embeddings_matrix)[0]\n",
        "\n",
        "    # Get indices of top 5 matches\n",
        "    top_5_indices = np.argsort(similarity_scores)[-5:]\n",
        "\n",
        "    # Append pairs to the list, including relevant prescription details\n",
        "    for index in top_5_indices:\n",
        "        # Create a dictionary with diagnosis and prescription data\n",
        "        prescription_data = prescriptions.iloc[index].to_dict()\n",
        "        pair = {**diagnosis_row, **prescription_data}\n",
        "        diagnosis_prescription_pairs.append(pair)\n"
      ],
      "metadata": {
        "id": "ItYLCZ9tQ4GC"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the function to each diagnosis\n",
        "for _, row in icd_10_diagnosis_used.iterrows():\n",
        "    find_top_5_matches(row['diagnosis_embedding'], row.to_dict())\n",
        "\n",
        "# Create a new DataFrame from the pairs\n",
        "diagnosis_prescription_df = pd.DataFrame(diagnosis_prescription_pairs)"
      ],
      "metadata": {
        "id": "JHyfii5iQ8s5"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drop embedding columns\n",
        "diagnosis_prescription_df = diagnosis_prescription_df.drop(['diagnosis_embedding', 'indication_embedding'], axis=1)\n"
      ],
      "metadata": {
        "id": "zOiovpNuBw5S"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert DataFrame to CSV\n",
        "diagnosis_prescription_df.to_csv('/content/diagnosis_prescription_before_human_rev.csv', index=False)\n",
        "\n",
        "# Initialize a client\n",
        "storage_client = storage.Client()\n",
        "bucket_name = 'scraped_data_clean'\n",
        "destination_blob_name = 'prescriptions/diagnosis_prescription_before_human_rev.csv'\n",
        "bucket = storage_client.bucket(bucket_name)\n",
        "blob = bucket.blob(destination_blob_name)\n",
        "blob.upload_from_filename('diagnosis_prescription_before_human_rev.csv')"
      ],
      "metadata": {
        "id": "oIl2pMZrTqx-"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data base is sent to human review and realoaded. Up to this point it is replicable."
      ],
      "metadata": {
        "id": "p4XbPZ1skVkZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#retrive the data base reviewed by a human (ideally, a doctor)\n",
        "bucket_name = 'scraped_data_clean'\n",
        "source_folder = \"prescriptions/\"\n",
        "destination_dir = \"/content/data\"\n",
        "download_folder(bucket_name, source_folder, destination_dir)\n",
        "\n",
        "reviewed_df = pd.read_csv('/content/data/diagnosis_prescription_human_reviewed.csv', encoding='latin1')\n",
        "\n",
        "#select the rows that, according to the human reviewer corresponf to drug and the indication\n",
        "reviewed_df = reviewed_df[reviewed_df['manual_review'] == 'x']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AIjOQnC8kOVf",
        "outputId": "a96748f6-448b-44ab-80bd-a30bf3514530"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Blob prescriptions/diagnosis_prescription_before_human_rev.csv downloaded to /content/data/diagnosis_prescription_before_human_rev.csv.\n",
            "Blob prescriptions/diagnosis_prescription_human_reviewed.csv downloaded to /content/data/diagnosis_prescription_human_reviewed.csv.\n",
            "Blob prescriptions/prescription_database.csv downloaded to /content/data/prescription_database.csv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reviewed_df = reviewed_df[['Original Code_x', 'Matched_Code', 'description_x', 'Drug Name',\n",
        "       'Dosing Information']]"
      ],
      "metadata": {
        "id": "D-d-hA31kOht"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Some final edits to on the strings\n",
        "reviewed_df['Drug Name'] = reviewed_df['Drug Name'].str.capitalize()\n",
        "reviewed_df['description_x'] = reviewed_df['description_x'].str.capitalize()"
      ],
      "metadata": {
        "id": "P7v7iTA6l9lM"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviewed_df.to_csv('/content/prescription_database.csv', index=False)\n",
        "\n",
        "# Initialize a client\n",
        "storage_client = storage.Client()\n",
        "bucket_name = 'scraped_data_clean'\n",
        "destination_blob_name = 'prescriptions/prescription_database.csv'\n",
        "bucket = storage_client.bucket(bucket_name)\n",
        "blob = bucket.blob(destination_blob_name)\n",
        "blob.upload_from_filename('prescription_database.csv')"
      ],
      "metadata": {
        "id": "mG8opJhLmSic"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}