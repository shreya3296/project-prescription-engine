# -*- coding: utf-8 -*-
"""api_py.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1R8Atyeyg8QdGKm1CBgXbzfbe6KAi-gko
"""

# Imports
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import torch
from transformers import BloomTokenizerFast, BloomForSequenceClassification
import os
from google.cloud import storage
import uvicorn

# key for GCP
os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'steel-climber-398320-634bf855feea.json'

# Model and Tokenizer setup
model_path = "/content/finetuned_model"
tokenizer = BloomTokenizerFast.from_pretrained(model_path, repo_type="local")
model = BloomForSequenceClassification.from_pretrained(model_path)
model.eval()

# Predict function
def predict(symptom_text):
    inputs = tokenizer(symptom_text, return_tensors="pt", max_length=512, truncation=True)
    with torch.no_grad():
        outputs = model(**inputs)
    logits = outputs.logits
    predicted_class_idx = torch.argmax(logits, dim=1).item()

    # decode the predicted class index back to the original class label
    ### LOAD LABEL ENCODER FROM BUCKET
    #predicted_label = label_encoder.inverse_transform([predicted_class_idx])[0]

    return predicted_class_idx

# FastAPI app
app = FastAPI()

# Request and Response models
class PredictionRequest(BaseModel):
    text: str

class PredictionResponse(BaseModel):
    label: str
    confidence: float

# Prediction endpoint
@app.post("/predict", response_model=PredictionResponse)
async def predict_endpoint(request: PredictionRequest):
    label = predict(request.text)
    confidence = 1.0  # Modify as necessary
    return PredictionResponse(label=label, confidence=confidence)

# Run the app
if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000, log_level="info")