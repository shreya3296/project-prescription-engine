# -*- coding: utf-8 -*-
"""model_py.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MRBKEmvx5AEfvxEFs1NkfEs6W3MjATei
"""

# Import necessary libraries
import os
import pandas as pd
import torch
from torch.utils.data import Dataset
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from transformers import (BloomTokenizerFast, BloomForSequenceClassification,
                          TrainingArguments, Trainer)
import wandb

# Initialize WandB
wandb.init(project="bloom_fine_tuning_v3", entity="ac215-the-prescribers")

# Load data from GCP
def load_data_from_gcp(bucket_name, file_name):
    from google.cloud import storage
    storage_client = storage.Client()
    bucket = storage_client.bucket(bucket_name)
    blob = bucket.blob(file_name)
    blob.download_to_filename(file_name)
    return pd.read_json(file_name, lines=True)

# Function to initialize the model and tokenizer
def initialize_model_and_tokenizer():
    tokenizer = BloomTokenizerFast.from_pretrained("bigscience/bloom-1b1", add_prefix_space=True)
    model = BloomForSequenceClassification.from_pretrained("bigscience/bloom-1b1")
    return tokenizer, model

# Function to tokenize data
def tokenize_data(tokenizer, data, labels, max_length=512):
    tokenized_data = tokenizer(data, max_length=max_length, truncation=True, padding=True, return_tensors="pt")
    tokenized_data['labels'] = torch.tensor(labels)
    return tokenized_data

# Custom Dataset class
class CustomDataset(Dataset):
    def __init__(self, tokenized_data):
        self.data = tokenized_data

    def __len__(self):
        return len(self.data['input_ids'])

    def __getitem__(self, idx):
        return {key: val[idx] for key, val in self.data.items()}

# Function to prepare datasets
def prepare_datasets(training_data, test_size=0.1):
    train_data, val_data, train_labels, val_labels = train_test_split(training_data['symptoms'].tolist(),
                                                                      training_data['label'].tolist(), test_size=test_size)
    return train_data, val_data, train_labels, val_labels

# Function to fine-tune the model
def fine_tune_model(model, train_dataset, val_dataset, training_args):
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset
    )
    trainer.train()
    return model

# Function to save the model
def save_model(model, tokenizer, save_directory):
    model.save_pretrained(save_directory)
    tokenizer.save_pretrained(save_directory)

def main():
    # Initialize tokenizer and model
    tokenizer, model = initialize_model_and_tokenizer()

    # Load data
    training_data = load_data_from_gcp('training_dataset_bloom', 'merged_symp_icd/symptoms_icd10.jsonl')
    num_labels = training_data['icd10_code'].nunique()

    # Encode labels
    label_encoder = LabelEncoder()
    training_data['label'] = label_encoder.fit_transform(training_data['icd10_code'])

    # Prepare datasets
    train_dataset, val_dataset, train_icd, val_icd = prepare_datasets(training_data)

    # Tokenize
    tokenized_train = tokenize_data(tokenizer, train_dataset, train_icd)
    tokenized_val = tokenize_data(tokenizer, val_dataset, val_icd)

    train_dataset = CustomDataset(tokenized_train)
    val_dataset = CustomDataset(tokenized_val)

    # Fine-tuning arguments
    training_args = TrainingArguments(
        output_dir="/content/local_results",
        per_device_train_batch_size=32,
        num_train_epochs=3,
        logging_steps=10,
        save_steps=100,
        evaluation_strategy="steps",
        save_total_limit=5,
        do_train=True,
        remove_unused_columns=False,
        logging_dir='wandb',
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        report_to='wandb',
    )

    # Fine-tune the model
    fine_tuned_model = fine_tune_model(model, train_dataset, val_dataset, training_args)

    # Save the model
    save_directory = "/content/finetuned_model"
    save_model(fine_tuned_model, tokenizer, save_directory)

if __name__ == "__main__":
    main()