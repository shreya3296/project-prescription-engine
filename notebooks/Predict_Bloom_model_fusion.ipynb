{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from google.cloud import storage\n",
        "from transformers import BloomTokenizerFast, BloomForCausalLM\n",
        "from transformers import BloomTokenizerFast\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "import json\n",
        "import torch.nn as nn\n",
        "from transformers import BloomForSequenceClassification\n",
        "from joblib import load"
      ],
      "metadata": {
        "id": "6OIfKovoGwUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/content/steel-climber-398320-634bf855feea.json'"
      ],
      "metadata": {
        "id": "kHt7s0zQGwM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_folder(bucket_name, source_folder, destination_dir):\n",
        "    \"\"\"Downloads a folder from the bucket.\"\"\"\n",
        "    storage_client = storage.Client('steel-climber-398320')  # Project of the bucket. Must be included\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "\n",
        "    # List all blobs in the bucket\n",
        "    blobs = bucket.list_blobs(prefix=source_folder)  # Add prefix to optimize listing\n",
        "\n",
        "    for blob in blobs:\n",
        "        if not blob.name.endswith('/'):  # Skip directories\n",
        "            # Construct the local file path\n",
        "            local_file_path = os.path.join(destination_dir, blob.name[len(source_folder):])\n",
        "            # Create local path folders\n",
        "            os.makedirs(os.path.dirname(local_file_path), exist_ok=True)\n",
        "            # Download file by file\n",
        "            blob.download_to_filename(local_file_path)\n",
        "            # Keep track\n",
        "            print(f\"Blob {blob.name} downloaded to {local_file_path}.\")\n",
        "\n",
        "# Get the model\n",
        "bucket_name = 'finetunned_760m_bloom'\n",
        "source_folder = \"bloom_model_fusion/\"\n",
        "destination_dir = \"/content/finetuned_model\"\n",
        "\n",
        "download_folder(bucket_name, source_folder, destination_dir)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BdhxH7fGwGo",
        "outputId": "a184c472-04c0-4062-80a0-8aabe3ce6734"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Blob bloom_model_fusion/custom_bloom_model.bin downloaded to /content/finetuned_model/custom_bloom_model.bin.\n",
            "Blob bloom_model_fusion/label_encoder_mapping.json downloaded to /content/finetuned_model/label_encoder_mapping.json.\n",
            "Blob bloom_model_fusion/preprocessor.joblib downloaded to /content/finetuned_model/preprocessor.joblib.\n",
            "Blob bloom_model_fusion/special_tokens_map.json downloaded to /content/finetuned_model/special_tokens_map.json.\n",
            "Blob bloom_model_fusion/tokenizer.json downloaded to /content/finetuned_model/tokenizer.json.\n",
            "Blob bloom_model_fusion/tokenizer_config.json downloaded to /content/finetuned_model/tokenizer_config.json.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#customBloomModel definition\n",
        "#we need to include this here, because the model architecture is not saved\n",
        "#and we are using a personalized model architecture to perform model fusion\n",
        "class CustomBloomModel(nn.Module):\n",
        "    def __init__(self, bloom_model, num_additional_features, num_labels):\n",
        "        super(CustomBloomModel, self).__init__()\n",
        "        self.bloom_model = bloom_model\n",
        "        self.num_labels = num_labels\n",
        "\n",
        "        # Define the additional neural network\n",
        "        self.additional_nn = nn.Sequential(\n",
        "            nn.Linear(51, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(64, num_labels)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, additional_features, labels=None):\n",
        "        model_output = self.bloom_model(input_ids, attention_mask=attention_mask)\n",
        "        logits = model_output.logits\n",
        "\n",
        "        # Concatenate Bloom model output with additional features\n",
        "        combined_features = torch.cat((logits, additional_features), dim=1)\n",
        "        final_logits = self.additional_nn(combined_features)\n",
        "\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(final_logits.view(-1, self.num_labels), labels.view(-1))\n",
        "            return loss, final_logits\n",
        "        else:\n",
        "            return final_logits\n",
        "\n",
        "\n",
        "#load the tokenizer\n",
        "tokenizer = BloomTokenizerFast.from_pretrained(\"/content/finetuned_model\")\n",
        "\n",
        "#load the OneHotEncoder\n",
        "preprocessor = load('/content/finetuned_model/preprocessor.joblib')\n",
        "\n",
        "num_additional_features = 3\n",
        "num_labels = 48\n",
        "\n",
        "#load the pre-trained BLOOM model\n",
        "bloom_pretrained_model = BloomForSequenceClassification.from_pretrained(\"bigscience/bloom-560m\", num_labels=num_labels)\n",
        "\n",
        "#load the Custom Model\n",
        "model_path = \"/content/finetuned_model/custom_bloom_model.bin\"\n",
        "custom_model = CustomBloomModel(bloom_model=bloom_pretrained_model, num_additional_features=num_additional_features, num_labels=num_labels)\n",
        "custom_model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
        "custom_model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2C_pCR1rKAK8",
        "outputId": "b04505e8-c497-41da-9e2c-4e74b9b7c6aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BloomForSequenceClassification were not initialized from the model checkpoint at bigscience/bloom-560m and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CustomBloomModel(\n",
              "  (bloom_model): BloomForSequenceClassification(\n",
              "    (transformer): BloomModel(\n",
              "      (word_embeddings): Embedding(250880, 1024)\n",
              "      (word_embeddings_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (h): ModuleList(\n",
              "        (0-23): 24 x BloomBlock(\n",
              "          (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (self_attention): BloomAttention(\n",
              "            (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): BloomMLP(\n",
              "            (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (gelu_impl): BloomGelu()\n",
              "            (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (score): Linear(in_features=1024, out_features=48, bias=False)\n",
              "  )\n",
              "  (additional_nn): Sequential(\n",
              "    (0): Linear(in_features=51, out_features=64, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Dropout(p=0.1, inplace=False)\n",
              "    (3): Linear(in_features=64, out_features=48, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_data = {\n",
        "    \"symptoms\": \"I have a head ache and have back pain\",\n",
        "    \"age\": 30,\n",
        "    \"gender\": \"M\"\n",
        "}\n",
        "\n",
        "# Dummy DataFrame for preprocessing\n",
        "df = pd.DataFrame([input_data])\n",
        "age_sex_data = preprocessor.transform(df[['age', 'gender']])\n",
        "\n",
        "# Tokenize symptoms\n",
        "max_length = 512  # Example length, adjust as needed\n",
        "tokenized_input = tokenizer(input_data[\"symptoms\"], return_tensors=\"pt\", padding=\"max_length\", max_length=max_length, truncation=True)\n",
        "\n",
        "# Since we are using CPU\n",
        "tokenized_input = {k: v for k, v in tokenized_input.items()}\n",
        "\n",
        "# Make prediction\n",
        "with torch.no_grad():\n",
        "    logits = custom_model(\n",
        "        input_ids=tokenized_input['input_ids'],\n",
        "        attention_mask=tokenized_input['attention_mask'],\n",
        "        additional_features=torch.tensor(age_sex_data, dtype=torch.float)\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clpYC-c8M83I",
        "outputId": "bddc3dca-5ffa-48b2-d34c-c65710d3dd99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-2.6361e+00, -4.9862e-01,  3.5639e-01, -4.0827e+00, -2.0414e+00,\n",
            "         -1.6602e+00, -1.6674e+00, -1.5698e-02, -6.7632e+00, -7.1057e+00,\n",
            "         -8.1212e+00, -1.9796e+00, -5.7114e+00,  1.1391e+00, -1.8258e+00,\n",
            "          1.2902e+00, -2.0986e+00, -1.4231e+00, -7.1882e+00,  3.1911e+00,\n",
            "          7.9038e+00,  8.2600e+00,  3.9708e+00, -7.1225e+00, -7.2012e+00,\n",
            "          1.2772e-03,  9.4341e-01, -4.8472e+00, -3.1579e+00, -4.3232e-01,\n",
            "         -1.9563e+00, -5.2866e+00, -1.5424e+00,  7.5999e+00, -3.8996e+00,\n",
            "         -1.5304e+00, -6.5662e+00, -4.7386e-01, -1.7548e+00,  1.2431e+00,\n",
            "         -3.8188e+00, -1.0732e+00, -1.8186e+00, -4.1354e+00, -1.2537e+01,\n",
            "         -1.2128e+00, -4.1330e+00,  5.0212e+00]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#load the label encoder mapping\n",
        "with open('/content/finetuned_model/label_encoder_mapping.json', 'r') as file:\n",
        "    label_encoder_mapping = json.load(file)\n",
        "\n",
        "#invert the mapping\n",
        "index_to_label_mapping = {v: k for k, v in label_encoder_mapping.items()}\n",
        "\n",
        "#calculate probabilities using softmax\n",
        "probabilities = torch.softmax(logits, dim=-1)\n",
        "\n",
        "#get the top 5 predictions\n",
        "top5_prob, top5_indices = torch.topk(probabilities, 5)\n",
        "top5_indices = top5_indices[0].tolist()\n",
        "top5_prob = top5_prob[0].tolist()\n",
        "\n",
        "#top 5 predictions and probs\n",
        "print(\"Top 5 likely diagnoses:\")\n",
        "for i in range(len(top5_indices)):\n",
        "    label = index_to_label_mapping[top5_indices[i]]\n",
        "    prob = top5_prob[i]\n",
        "    print(f\"{i+1}: ICD-10 Code: {label}, Probability: {prob:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FsLCyh_ZOtLS",
        "outputId": "93d7321a-6597-44cc-f311-ffdaf2d9a0cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 likely diagnoses:\n",
            "1: ICD-10 Code: J45, Probability: 0.4382\n",
            "2: ICD-10 Code: J38.5, Probability: 0.3069\n",
            "3: ICD-10 Code: a15, Probability: 0.2265\n",
            "4: ICD-10 Code: j44.1, Probability: 0.0172\n",
            "5: ICD-10 Code: J47, Probability: 0.0060\n"
          ]
        }
      ]
    }
  ]
}