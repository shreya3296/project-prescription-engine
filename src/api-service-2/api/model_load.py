# -*- coding: utf-8 -*-
"""model_load.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tiBk1n48NMuUztDf_COwoZVqjvM_xb7P
"""

from pathlib import Path
from transformers import BloomTokenizerFast, BloomForSequenceClassification
import pandas as pd
import torch
import torch.nn as nn
import numpy as np
import json
import joblib

#customBloomModel definition
#we need to include this here, because the model architecture is not saved
#and we are using a personalized model architecture to perform model fusion
class CustomBloomModel(nn.Module):
    def __init__(self, bloom_model, num_additional_features, num_labels):
        super(CustomBloomModel, self).__init__()
        self.bloom_model = bloom_model
        self.num_labels = num_labels

        # Define the additional neural network
        self.additional_nn = nn.Sequential(
            nn.Linear(51, 64),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(64, num_labels)
        )

    def forward(self, input_ids, attention_mask, additional_features, labels=None):
        model_output = self.bloom_model(input_ids, attention_mask=attention_mask)
        logits = model_output.logits

        # Concatenate Bloom model output with additional features
        combined_features = torch.cat((logits, additional_features), dim=1)
        final_logits = self.additional_nn(combined_features)

        if labels is not None:
            loss_fct = nn.CrossEntropyLoss()
            loss = loss_fct(final_logits.view(-1, self.num_labels), labels.view(-1))
            return loss, final_logits
        else:
            return final_logits

# assume model is in a directory called "model" in the current working directory
model_path = Path("model")
num_additional_features = 3
num_labels = 48

icd10 = pd.read_csv("prescription_database.tsv", sep="\t")
tokenizer = BloomTokenizerFast.from_pretrained(model_path, repo_type="local")
preprocessor = joblib.load(model_path / "preprocessor.joblib")
bloom_pretrained_model = BloomForSequenceClassification.from_pretrained("bigscience/bloom-560m", num_labels=48)
custom_model = CustomBloomModel(bloom_model=bloom_pretrained_model, num_additional_features=num_additional_features, num_labels=num_labels)
custom_model.load_state_dict(torch.load(model_path / "custom_bloom_model.bin", map_location=torch.device('cpu')))

custom_model.eval()

#load the label encoder mapping (assume in the same directory as the model)
with open(model_path / "label_encoder_mapping.json", 'r') as file:
    label_encoder_mapping = json.load(file)

#invert the mapping
index_to_label_mapping = {v: k for k, v in label_encoder_mapping.items()}

def tokenize_input(prompt):
    # Tokenize the input
    tokenized_input = tokenizer(prompt, return_tensors="pt", padding="max_length", max_length=512, truncation=True)
    return {k: v for k,v in tokenized_input.items()}

def run (input_data):
    df = pd.DataFrame([input_data])
    age_sex_data = preprocessor.transform(df[['age', 'gender']])

    tokenized_input = tokenize_input(input_data["symptoms"])

    # get the model's prediction
    with torch.no_grad():
        logits = custom_model(
          input_ids=tokenized_input['input_ids'],
          attention_mask=tokenized_input['attention_mask'],
          additional_features=torch.tensor(age_sex_data, dtype=torch.float)
        )

    probabilities = torch.softmax(logits, dim=-1)

    predicted_class_idx = torch.argmax(logits, dim=1).item()

    top5_prob, top5_indices = torch.topk(probabilities, 5)

    # get list of ICD-10 entries from TSV
    icd10_codes = (icd10[icd10["Original Code_x"] == index_to_label_mapping[i]] for i in top5_indices[0].tolist())

    # return list of (icd10, probability, description) tuples
    return [(diagnosis.Matched_Code.iloc[0],
             probability,
             diagnosis.description_x.iloc[0],
             list(diagnosis.groupby(("Drug Name"))["Dosing Information"].apply(list).items()))
             for diagnosis, probability in zip(icd10_codes, top5_prob[0].tolist())]